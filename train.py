from model import MaxVit_Model
from preprocess import image_augumentation, my_Dataset
from torch.utils.data import DataLoader
from utils import seed_everything, load_collections, split_ds
from yaml import safe_load
import torch
import torch.optim as optim
import torch.nn as nn
from torch.optim.lr_scheduler import StepLR
from tqdm import tqdm
import argparse


def load_yml_file(file_path):
    with open(file_path) as f:
        yaml_data = safe_load(f)
    return yaml_data

def load_dataset(ds_path, train_config):

    corpus, labels = load_collections(ds_path)

    print('\ngenerating dataset...')
    # calculate size
    test_size = train_config['test_size']
    val_size = test_size / (1 - test_size)
    seed = train_config['seed']
    
    # split samples
    X_train, X_test, y_train, y_test  = split_ds(corpus, labels, test_size, seed)
    X_train, X_val, y_train, y_val  = split_ds(X_train, y_train, val_size, seed + 1)

    # add augumentaion
    train_transforms, val_transforms, test_transforms = image_augumentation()

    train_data = my_Dataset(X_train, y_train, transform=train_transforms)
    valid_data = my_Dataset(X_val, y_val, transform=val_transforms)
    test_data = my_Dataset(X_test,y_test, transform=test_transforms)

    batch_size = train_config['batch_size']
    train_loader = DataLoader(dataset = train_data, batch_size=batch_size, shuffle=True )
    valid_loader = DataLoader(dataset = valid_data, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(dataset = test_data, batch_size=batch_size, shuffle=True)

    print(f"    Train data: ({len(train_data)}, {len(train_loader)})")
    print(f"    Val data: ({len(valid_data)}, {len(valid_loader)})")
    print(f"    Test data: ({len(test_data)}, {len(test_loader)})")

    return train_loader, valid_loader, test_loader

def my_train(model, epochs):

    print('start training...')
    for epoch in range(epochs):
        epoch_loss = 0
        epoch_accuracy = 0

        for data, label in tqdm(train_loader):
            data = data.to(device)
            label = label.to(device)

            output = model(data)
            loss = criterion(output, label)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            acc = (output.argmax(dim=1) == label).float().mean()
            epoch_accuracy += acc / len(train_loader)
            epoch_loss += loss / len(train_loader)

        with torch.no_grad():
            epoch_val_accuracy = 0
            epoch_val_loss = 0
            for data, label in valid_loader:
                data = data.to(device)
                label = label.to(device)

                val_output = model(data)
                val_loss = criterion(val_output, label)

                acc = (val_output.argmax(dim=1) == label).float().mean()
                epoch_val_accuracy += acc / len(valid_loader)
                epoch_val_loss += val_loss / len(valid_loader)

        print(
            f"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\n"
        )


if __name__ == '__main__':

    # load parser
    parser = argparse.ArgumentParser(description='Malware detection with MaxViT transformer model')
    parser.add_argument('-t',metavar="type", choices=['S','L'], help='select MaxViT model type [S,L]')

    args = parser.parse_args()

    # load config file
    train_config = load_yml_file('config/train_config.yml')
    model_config = load_yml_file('config/model_config.yml')

    # set device
    print(f"Torch: {torch.__version__}")
    device = train_config['device']

    # seed
    seed_everything(train_config['seed'])

    # generate dataset from color img samples
    train_loader, valid_loader, test_loader = load_dataset(train_config['ds_path'], train_config)

    #create model
    model_type = 'MaxViT-'+ args.t
    print('\nusing model ',model_type)
    maxvit = MaxVit_Model(model_config[model_type], device)
    model = maxvit.model()

    # loss function
    criterion = nn.CrossEntropyLoss()
    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=train_config['lr'])
    # scheduler
    scheduler = StepLR(optimizer, step_size=1, gamma=train_config['gamma'])

    # start traning
    my_train(model, train_config['epochs'])